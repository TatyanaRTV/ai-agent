"""
–ö–æ–º–ø–æ–Ω–µ–Ω—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ—á–∏—Å—Ç–∫–∏ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
"""

import os
import shutil
import logging
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Any
import json

logger = logging.getLogger(__name__)

class AutoCleanup:
    """–°–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ—á–∏—Å—Ç–∫–∏ –Ω–µ–Ω—É–∂–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
    
    def __init__(self, config):
        self.config = config
        self.cleanup_history = []
        self.cleanup_rules = self._load_cleanup_rules()
        
    async def perform_cleanup(self, cleanup_type: str = "scheduled") -> Dict[str, Any]:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –æ—á–∏—Å—Ç–∫–∏"""
        logger.info(f"üßπ –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –æ—á–∏—Å—Ç–∫–∏ —Ç–∏–ø–∞: {cleanup_type}")
        
        cleanup_result = {
            "type": cleanup_type,
            "timestamp": datetime.now().isoformat(),
            "cleaned_items": [],
            "freed_space": 0,
            "errors": [],
            "duration": None
        }
        
        start_time = datetime.now()
        
        try:
            # 1. –û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
            temp_cleanup = await self._clean_temp_files()
            cleanup_result["cleaned_items"].extend(temp_cleanup)
            
            # 2. –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞
            cache_cleanup = await self._clean_cache()
            cleanup_result["cleaned_items"].extend(cache_cleanup)
            
            # 3. –û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –ª–æ–≥–æ–≤
            log_cleanup = await self._clean_old_logs()
            cleanup_result["cleaned_items"].extend(log_cleanup)
            
            # 4. –û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            data_cleanup = await self._clean_old_data()
            cleanup_result["cleaned_items"].extend(data_cleanup)
            
            # 5. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö
            optimization_result = await self._optimize_databases()
            cleanup_result["cleaned_items"].append(optimization_result)
            
            # 6. –û—á–∏—Å—Ç–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
            duplicate_cleanup = await self._remove_duplicates()
            cleanup_result["cleaned_items"].extend(duplicate_cleanup)
            
            # –†–∞—Å—á–µ—Ç –≤—ã—Å–≤–æ–±–æ–∂–¥–µ–Ω–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞
            cleanup_result["freed_space"] = self._calculate_freed_space(cleanup_result["cleaned_items"])
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            await self._save_cleanup_result(cleanup_result)
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏
            self.cleanup_history.append({
                "timestamp": datetime.now().isoformat(),
                "type": cleanup_type,
                "items_cleaned": len(cleanup_result["cleaned_items"]),
                "space_freed": cleanup_result["freed_space"]
            })
            
        except Exception as e:
            cleanup_result["errors"].append(str(e))
            logger.error(f"–û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –æ—á–∏—Å—Ç–∫–∏: {e}")
            
        end_time = datetime.now()
        cleanup_result["duration"] = (end_time - start_time).total_seconds()
        
        logger.info(f"‚úÖ –û—á–∏—Å—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –û—Å–≤–æ–±–æ–∂–¥–µ–Ω–æ: {cleanup_result['freed_space']} MB")
        
        return cleanup_result
        
    async def _clean_temp_files(self) -> List[Dict[str, Any]]:
        """–û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤"""
        cleaned_items = []
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—É—Ç–µ–π –¥–ª—è –æ—á–∏—Å—Ç–∫–∏
        temp_paths = [
            Path("data/temp"),
            Path("/tmp"),
            Path.home() / ".cache",
            Path("__pycache__"),
            Path(".pytest_cache")
        ]
        
        for temp_path in temp_paths:
            if temp_path.exists():
                items_cleaned = await self._clean_directory(temp_path, days_old=1)
                cleaned_items.extend(items_cleaned)
                
        return cleaned_items
        
    async def _clean_cache(self) -> List[Dict[str, Any]]:
        """–û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞"""
        cleaned_items = []
        
        cache_paths = [
            Path("data/cache"),
            Path("models/.cache"),
            Path(".cache")
        ]
        
        for cache_path in cache_paths:
            if cache_path.exists():
                items_cleaned = await self._clean_directory(cache_path, days_old=7)
                cleaned_items.extend(items_cleaned)
                
        return cleaned_items
        
    async def _clean_old_logs(self) -> List[Dict[str, Any]]:
        """–û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –ª–æ–≥–æ–≤"""
        cleaned_items = []
        
        log_paths = [
            Path("logs"),
            Path("data/logs")
        ]
        
        for log_path in log_paths:
            if log_path.exists():
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª–æ–≥–∏ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –¥–Ω–µ–π
                items_cleaned = await self._clean_directory(log_path, days_old=30)
                cleaned_items.extend(items_cleaned)
                
        return cleaned_items
        
    async def _clean_old_data(self) -> List[Dict[str, Any]]:
        """–û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        cleaned_items = []
        
        data_paths = [
            Path("data/raw"),
            Path("data/processed")
        ]
        
        for data_path in data_paths:
            if data_path.exists():
                # –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ä—à–µ 90 –¥–Ω–µ–π
                items_cleaned = await self._clean_directory(data_path, days_old=90)
                cleaned_items.extend(items_cleaned)
                
        return cleaned_items
        
    async def _optimize_databases(self) -> Dict[str, Any]:
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö"""
        optimization_result = {
            "type": "database_optimization",
            "timestamp": datetime.now().isoformat(),
            "actions": [],
            "space_saved": 0
        }
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
        try:
            # –ó–¥–µ—Å—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ª–æ–≥–∏–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –ë–î
            optimization_result["actions"].append("vector_db_vacuumed")
            optimization_result["space_saved"] += 100  # –ü—Ä–∏–º–µ—Ä–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ë–î: {e}")
            
        return optimization_result
        
    async def _remove_duplicates(self) -> List[Dict[str, Any]]:
        """–£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Ñ–∞–π–ª–æ–≤"""
        cleaned_items = []
        
        # –ü–æ–∏—Å–∫ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –≤ –¥–∞–Ω–Ω—ã—Ö
        data_dirs = ["data/raw", "data/processed", "data/vectors"]
        
        for data_dir in data_dirs:
            dir_path = Path(data_dir)
            if dir_path.exists():
                duplicates = await self._find_duplicates(dir_path)
                
                for duplicate in duplicates:
                    try:
                        duplicate.unlink()
                        cleaned_items.append({
                            "type": "duplicate_file",
                            "path": str(duplicate),
                            "size": duplicate.stat().st_size
                        })
                    except Exception as e:
                        logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å –¥—É–±–ª–∏–∫–∞—Ç {duplicate}: {e}")
                        
        return cleaned_items
        
    async def _clean_directory(self, directory: Path, days_old: int) -> List[Dict[str, Any]]:
        """–û—á–∏—Å—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å—Ç–∞—Ä—à–µ —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–Ω–µ–π"""
        cleaned_items = []
        cutoff_date = datetime.now() - timedelta(days=days_old)
        
        try:
            for item in directory.rglob("*"):
                if item.is_file():
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ä–µ–º–µ–Ω–∏ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏
                    mtime = datetime.fromtimestamp(item.stat().st_mtime)
                    
                    if mtime < cutoff_date:
                        try:
                            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∏—Å–∫–ª—é—á–µ–Ω–∏—è
                            if not self._is_excluded(item):
                                # –£–¥–∞–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
                                file_size = item.stat().st_size
                                item.unlink()
                                
                                cleaned_items.append({
                                    "type": "file",
                                    "path": str(item),
                                    "age_days": (datetime.now() - mtime).days,
                                    "size": file_size
                                })
                                
                        except Exception as e:
                            logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å —Ñ–∞–π–ª {item}: {e}")
                            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ {directory}: {e}")
            
        return cleaned_items
        
    def _is_excluded(self, file_path: Path) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ñ–∞–π–ª –∏—Å–∫–ª—é—á–µ–Ω–∏–µ–º"""
        excluded_patterns = [
            "*.config",
            "*.json",
            "*.yaml",
            "*.yml",
            "README*",
            ".gitkeep",
            ".gitignore"
        ]
        
        file_name = file_path.name
        
        for pattern in excluded_patterns:
            if pattern.startswith("*."):
                extension = pattern[1:]
                if file_name.endswith(extension):
                    return True
            elif pattern in file_name:
                return True
                
        return False
        
    def _calculate_freed_space(self, cleaned_items: List[Dict[str, Any]]) -> int:
        """–†–∞—Å—á–µ—Ç –≤—ã—Å–≤–æ–±–æ–∂–¥–µ–Ω–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –≤ MB"""
        total_bytes = sum(item.get("size", 0) for item in cleaned_items)
        return total_bytes // (1024 * 1024)  # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ MB
        
    async def _save_cleanup_result(self, result: Dict[str, Any]):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –æ—á–∏—Å—Ç–∫–∏"""
        cleanup_log_dir = Path("logs/cleanup")
        cleanup_log_dir.mkdir(exist_ok=True)
        
        log_file = cleanup_log_dir / f"cleanup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(log_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
            
    def _load_cleanup_rules(self) -> Dict[str, Any]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–∞–≤–∏–ª –æ—á–∏—Å—Ç–∫–∏"""
        default_rules = {
            "temp_files": {
                "enabled": True,
                "max_age_days": 1,
                "exclude_patterns": [".keep", "*.lock"]
            },
            "cache": {
                "enabled": True,
                "max_age_days": 7,
                "max_size_mb": 1024
            },
            "logs": {
                "enabled": True,
                "max_age_days": 30,
                "keep_min_count": 10
            },
            "data": {
                "enabled": True,
                "max_age_days": 90,
                "preserve_important": True
            }
        }
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –ø—Ä–∞–≤–∏–ª
        rules_file = Path("configs/cleanup_rules.json")
        if rules_file.exists():
            try:
                with open(rules_file, 'r', encoding='utf-8') as f:
                    user_rules = json.load(f)
                    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –ø—Ä–∞–≤–∏–ª–∞–º–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
                    default_rules.update(user_rules)
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø—Ä–∞–≤–∏–ª –æ—á–∏—Å—Ç–∫–∏: {e}")
                
        return default_rules
        
    async def _find_duplicates(self, directory: Path) -> List[Path]:
        """–ü–æ–∏—Å–∫ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Ñ–∞–π–ª–æ–≤"""
        import hashlib
        
        file_hashes = {}
        duplicates = []
        
        for file_path in directory.rglob("*"):
            if file_path.is_file():
                try:
                    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ö–µ—à–∞ —Ñ–∞–π–ª–∞
                    file_hash = self._calculate_file_hash(file_path)
                    
                    if file_hash in file_hashes:
                        # –ù–∞–π–¥–µ–Ω –¥—É–±–ª–∏–∫–∞—Ç
                        duplicates.append(file_path)
                    else:
                        file_hashes[file_hash] = file_path
                        
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ {file_path}: {e}")
                    
        return duplicates
        
    def _calculate_file_hash(self, file_path: Path) -> str:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ö–µ—à–∞ —Ñ–∞–π–ª–∞"""
        hash_md5 = hashlib.md5()
        
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
                
        return hash_md5.hexdigest()
        
    def get_cleanup_statistics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –æ—á–∏—Å—Ç–∫–∏"""
        if not self.cleanup_history:
            return {"total_cleanups": 0, "total_space_freed": 0}
            
        total_cleanups = len(self.cleanup_history)
        total_space_freed = sum(item.get("space_freed", 0) for item in self.cleanup_history)
        
        last_cleanup = self.cleanup_history[-1] if self.cleanup_history else {}
        
        return {
            "total_cleanups": total_cleanups,
            "total_space_freed": f"{total_space_freed} MB",
            "last_cleanup": last_cleanup.get("timestamp"),
            "last_cleanup_items": last_cleanup.get("items_cleaned", 0),
            "average_items_per_cleanup": total_cleanups / len(self.cleanup_history) if self.cleanup_history else 0
        }
        
    async def schedule_cleanup(self, interval_hours: int = 24):
        """–ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–≥—É–ª—è—Ä–Ω–æ–π –æ—á–∏—Å—Ç–∫–∏"""
        import asyncio
        
        logger.info(f"‚è∞ –ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—á–∏—Å—Ç–∫–∏ –∫–∞–∂–¥—ã–µ {interval_hours} —á–∞—Å–æ–≤")
        
        while True:
            try:
                # –û–∂–∏–¥–∞–Ω–∏–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞
                await asyncio.sleep(interval_hours * 3600)
                
                # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –æ—á–∏—Å—Ç–∫–∏
                await self.perform_cleanup("scheduled")
                
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –≤ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–µ –æ—á–∏—Å—Ç–∫–∏: {e}")
                await asyncio.sleep(3600)  # –û–∂–∏–¥–∞–Ω–∏–µ –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–æ–π