#!/usr/bin/env python3
# –ü—É—Ç—å: /mnt/ai_data/ai-agent/src/engines/vision_engine.py
"""–ó—Ä–∏—Ç–µ–ª—å–Ω—ã–π –º–æ–¥—É–ª—å –ï–ª–µ–Ω—ã –Ω–∞ –±–∞–∑–µ nanoLLaVA - —Å—Ç–∞–±–∏–ª—å–Ω–∞—è –∏ –ª–µ–≥–∫–∞—è –º–æ–¥–µ–ª—å"""

import mss  # type: ignore[import-untyped]
import pyautogui  # type: ignore[import-untyped]
import asyncio
from PIL import Image
from pathlib import Path
from datetime import datetime
import torch
import transformers  # type: ignore[import-untyped]
from transformers import AutoModelForCausalLM, AutoTokenizer  # type: ignore[import-untyped]
from loguru import logger
import warnings
from typing import Any, Optional, Dict, List, Union, cast, Tuple

# –û—Ç–∫–ª—é—á–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
if hasattr(transformers, "logging"):
    cast(Any, transformers).logging.set_verbosity_error()
    cast(Any, transformers).logging.disable_progress_bar()
warnings.filterwarnings("ignore")


class VisionEngine:
    """
    –ó—Ä–∏—Ç–µ–ª—å–Ω—ã–π –¥–≤–∏–∂–æ–∫ –ï–ª–µ–Ω—ã –Ω–∞ –±–∞–∑–µ nanoLLaVA.
    –ù–∞–¥–µ–∂–Ω–∞—è —Ä–∞–±–æ—Ç–∞ —Å –∞–∫—Ç—É–∞–ª—å–Ω—ã–º–∏ –≤–µ—Ä—Å–∏—è–º–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫.
    """

    def __init__(self, config: Dict[str, Any]) -> None:
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –¥–≤–∏–∂–∫–∞

        Args:
            config: —Å–ª–æ–≤–∞—Ä—å —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
        """
        self.config = config
        self.device = self._get_device()

        # –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤
        self.screenshot_dir = Path(config["paths"]["data"]) / "screenshots"
        self.screenshot_dir.mkdir(parents=True, exist_ok=True)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞—Ö–≤–∞—Ç–∞ —ç–∫—Ä–∞–Ω–∞
        self.sct = mss.mss()

        # –ú–æ–¥–µ–ª—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        self.model: Any = None
        self.tokenizer: Any = None

        # –ó–∞–≥—Ä—É–∂–∞–µ–º nanoLLaVA
        self._try_load_nanollava()

        if self.model:
            logger.success(f"‚úÖ nanoLLaVA –∑–∞–≥—Ä—É–∂–µ–Ω –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ {self.device}")
        else:
            logger.warning("üëÅÔ∏è nanoLLaVA –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω (—Å–∫—Ä–∏–Ω—à–æ—Ç—ã –±–µ–∑ –∞–Ω–∞–ª–∏–∑–∞)")

    def _get_device(self) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–æ—Å—Ç—É–ø–Ω–æ–≥–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞"""
        if torch.cuda.is_available():
            logger.info("üöÄ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CUDA (GPU)")
            return "cuda"
        else:
            logger.info("üíª –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU")
            return "cpu"

    def _try_load_nanollava(self) -> None:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ nanoLLaVA"""
        try:
            model_name = "qnguyen3/nanoLLaVA"

            logger.info(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ nanoLLaVA...")

            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è torch
            torch.set_default_device(self.device)

            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                device_map="auto" if self.device == "cuda" else None,
                trust_remote_code=True,
            )

            self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)

            logger.success("‚úÖ nanoLLaVA –∑–∞–≥—Ä—É–∂–µ–Ω —É—Å–ø–µ—à–Ω–æ")

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è nanoLLaVA –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω: {e}")
            self.model = None
            self.tokenizer = None

    async def capture_screen(self, delay: int = 5) -> Optional[Image.Image]:
        """
        –°–¥–µ–ª–∞—Ç—å —Å–∫—Ä–∏–Ω—à–æ—Ç —ç–∫—Ä–∞–Ω–∞ –º–æ–Ω–∏—Ç–æ—Ä–∞, –Ω–∞ –∫–æ—Ç–æ—Ä–æ–º –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –∫—É—Ä—Å–æ—Ä –º—ã—à–∏

        Args:
            delay: –∑–∞–¥–µ—Ä–∂–∫–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö –ø–µ—Ä–µ–¥ —Å–Ω–∏–º–∫–æ–º

        Returns:
            PIL Image –∏–ª–∏ None
        """
        try:
            logger.info(f"‚è≥ –ó–∞–¥–µ—Ä–∂–∫–∞ {delay} —Å–µ–∫—É–Ω–¥ –ø–µ—Ä–µ–¥ —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–º...")
            await asyncio.sleep(delay)

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ–∑–∏—Ü–∏—é –º—ã—à–∏ —á–µ—Ä–µ–∑ pyautogui
            mouse_x, mouse_y = pyautogui.position()

            # –ü–æ–∏—Å–∫ –º–æ–Ω–∏—Ç–æ—Ä–∞, –≥–¥–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –∫—É—Ä—Å–æ—Ä (–ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç sct.monitors[0])
            target_monitor = self.sct.monitors[0]
            for i, monitor in enumerate(self.sct.monitors[1:], 1):
                if (
                    monitor["left"] <= mouse_x < monitor["left"] + monitor["width"]
                    and monitor["top"] <= mouse_y < monitor["top"] + monitor["height"]
                ):
                    target_monitor = monitor
                    logger.info(f"üñ•Ô∏è –ó–∞—Ö–≤–∞—Ç –º–æ–Ω–∏—Ç–æ—Ä–∞ ‚Ññ{i} (–º—ã—à—å –Ω–∞ x={mouse_x}, y={mouse_y})")
                    break

            screenshot = self.sct.grab(target_monitor)
            img = Image.frombytes("RGB", screenshot.size, screenshot.rgb)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è –∏—Å—Ç–æ—Ä–∏–∏
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            save_path = self.screenshot_dir / f"screenshot_{timestamp}.png"
            img.save(save_path)
            logger.debug(f"üì∏ –°–∫—Ä–∏–Ω—à–æ—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {save_path}")

            return img
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞—Ö–≤–∞—Ç–∞ —ç–∫—Ä–∞–Ω–∞: {e}")
            return None

    def describe(self, image: Image.Image, prompt: str = "–û–ø–∏—à–∏ —ç—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–æ–¥—Ä–æ–±–Ω–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ") -> str:
        """
        –ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é nanoLLaVA
        """
        if self.model is None or self.tokenizer is None:
            return self._basic_description(image)

        try:
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ ChatML
            messages = [{"role": "user", "content": f"<image>\n{prompt}"}]

            text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

            # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ —Ç–æ–∫–µ–Ω—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            text_chunks = [self.tokenizer(chunk).input_ids for chunk in text.split("<image>")]
            input_ids = torch.tensor(text_chunks[0] + [-200] + text_chunks[1], dtype=torch.long).unsqueeze(0)

            if self.device == "cuda":
                input_ids = input_ids.cuda()

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            model_any = cast(Any, self.model)
            image_tensor = model_any.process_images([image], model_any.config).to(dtype=model_any.dtype)
            if self.device == "cuda":
                image_tensor = image_tensor.cuda()

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            with torch.no_grad():
                output_ids = model_any.generate(
                    input_ids, images=image_tensor, max_new_tokens=200, use_cache=True, temperature=0.7, do_sample=True
                )[0]

            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            answer = self.tokenizer.decode(output_ids[input_ids.shape[1] :], skip_special_tokens=True).strip()

            logger.info(f"üìù nanoLLaVA: {answer[:100]}...")
            return cast(str, answer)

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
            return self._basic_description(image)

    def _basic_description(self, image: Image.Image) -> str:
        """–ë–∞–∑–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –±–µ–∑ ML –º–æ–¥–µ–ª–∏"""
        try:
            # –Ø–≤–Ω–∞—è —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∞ –¥–ª—è MyPy
            size: Tuple[int, int] = image.size
            width, height = size
            mode = image.mode

            desc = f"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–º {width}x{height} –ø–∏–∫—Å–µ–ª–µ–π, {mode}"
            if mode == "L":
                desc += ", —á—ë—Ä–Ω–æ-–±–µ–ª–æ–µ"
            elif mode == "RGB":
                desc += ", —Ü–≤–µ—Ç–Ω–æ–µ"
            elif mode == "RGBA":
                desc += ", —Ü–≤–µ—Ç–Ω–æ–µ —Å –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç—å—é"

            return desc
        except Exception:
            return "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏"

    def unload_model(self) -> None:
        """–í—ã–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ –ø–∞–º—è—Ç–∏"""
        if self.model is not None:
            self.model = self.model.cpu()
            del self.model
            del self.tokenizer
            self.model = None
            self.tokenizer = None

            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()

            import gc

            gc.collect()
            logger.info("üßπ nanoLLaVA –≤—ã–≥—Ä—É–∂–µ–Ω –∏–∑ –ø–∞–º—è—Ç–∏")

    def is_model_loaded(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –ª–∏ –º–æ–¥–µ–ª—å"""
        return self.model is not None and self.tokenizer is not None
