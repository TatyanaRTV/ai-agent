#!/usr/bin/env python3
# –ü—É—Ç—å: /mnt/ai_data/ai-agent/src/engines/vision_engine.py
"""–ó—Ä–∏—Ç–µ–ª—å–Ω—ã–π –º–æ–¥—É–ª—å –ï–ª–µ–Ω—ã –Ω–∞ –±–∞–∑–µ nanoLLaVA - —Å—Ç–∞–±–∏–ª—å–Ω–∞—è –∏ –ª–µ–≥–∫–∞—è –º–æ–¥–µ–ª—å"""

import mss
from PIL import Image
from pathlib import Path
from datetime import datetime
import torch
import transformers
from transformers import AutoModelForCausalLM, AutoTokenizer
from loguru import logger
import warnings

# –û—Ç–∫–ª—é—á–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
transformers.logging.set_verbosity_error()
transformers.logging.disable_progress_bar()
warnings.filterwarnings("ignore")


class VisionEngine:
    """
    –ó—Ä–∏—Ç–µ–ª—å–Ω—ã–π –¥–≤–∏–∂–æ–∫ –ï–ª–µ–Ω—ã –Ω–∞ –±–∞–∑–µ nanoLLaVA.
    –ù–∞–¥–µ–∂–Ω–∞—è —Ä–∞–±–æ—Ç–∞ —Å –∞–∫—Ç—É–∞–ª—å–Ω—ã–º–∏ –≤–µ—Ä—Å–∏—è–º–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫.
    """

    def __init__(self, config):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –¥–≤–∏–∂–∫–∞

        Args:
            config: —Å–ª–æ–≤–∞—Ä—å —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
        """
        self.config = config
        self.device = self._get_device()

        # –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤
        self.screenshot_dir = Path(config["paths"]["data"]) / "screenshots"
        self.screenshot_dir.mkdir(parents=True, exist_ok=True)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞—Ö–≤–∞—Ç–∞ —ç–∫—Ä–∞–Ω–∞
        self.sct = mss.mss()

        # –ú–æ–¥–µ–ª—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        self.model = None
        self.tokenizer = None

        # –ó–∞–≥—Ä—É–∂–∞–µ–º nanoLLaVA
        self._try_load_nanollava()

        if self.model:
            logger.success(f"‚úÖ nanoLLaVA –∑–∞–≥—Ä—É–∂–µ–Ω –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ {self.device}")
        else:
            logger.warning("üëÅÔ∏è nanoLLaVA –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω (—Å–∫—Ä–∏–Ω—à–æ—Ç—ã –±–µ–∑ –∞–Ω–∞–ª–∏–∑–∞)")

    def _get_device(self):
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–æ—Å—Ç—É–ø–Ω–æ–≥–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞"""
        if torch.cuda.is_available():
            logger.info("üöÄ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CUDA (GPU)")
            return "cuda"
        else:
            logger.info("üíª –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU")
            return "cpu"

    def _try_load_nanollava(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ nanoLLaVA"""
        try:
            model_name = "qnguyen3/nanoLLaVA"  # –∏–ª–∏ "qnguyen3/nanoLLaVA-1.5" –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–∏

            logger.info(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ nanoLLaVA...")

            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è torch
            torch.set_default_device(self.device)

            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                device_map="auto" if self.device == "cuda" else None,
                trust_remote_code=True,
            )

            self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)

            logger.success("‚úÖ nanoLLaVA –∑–∞–≥—Ä—É–∂–µ–Ω —É—Å–ø–µ—à–Ω–æ")

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è nanoLLaVA –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω: {e}")
            self.model = None
            self.tokenizer = None

    def capture_screen(self, monitor=1):
        """
        –°–¥–µ–ª–∞—Ç—å —Å–∫—Ä–∏–Ω—à–æ—Ç —ç–∫—Ä–∞–Ω–∞

        Args:
            monitor: –Ω–æ–º–µ—Ä –º–æ–Ω–∏—Ç–æ—Ä–∞ (1, 2, ...)

        Returns:
            PIL Image –∏–ª–∏ None
        """
        try:
            screenshot = self.sct.grab(self.sct.monitors[monitor])
            img = Image.frombytes("RGB", screenshot.size, screenshot.rgb)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è –∏—Å—Ç–æ—Ä–∏–∏
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            save_path = self.screenshot_dir / f"screenshot_{timestamp}.png"
            img.save(save_path)
            logger.debug(f"üì∏ –°–∫—Ä–∏–Ω—à–æ—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {save_path}")

            return img

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞—Ö–≤–∞—Ç–∞ —ç–∫—Ä–∞–Ω–∞: {e}")
            return None

    def describe(self, image: Image.Image, prompt: str = "–û–ø–∏—à–∏ —ç—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–æ–¥—Ä–æ–±–Ω–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ"):
        """
        –ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é nanoLLaVA

        Args:
            image: PIL Image
            prompt: –∑–∞–ø—Ä–æ—Å –¥–ª—è –æ–ø–∏—Å–∞–Ω–∏—è

        Returns:
            –æ–ø–∏—Å–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        """
        if self.model is None or self.tokenizer is None:
            return self._basic_description(image)

        try:
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ ChatML
            messages = [{"role": "user", "content": f"<image>\n{prompt}"}]

            text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

            # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ —Ç–æ–∫–µ–Ω—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            text_chunks = [self.tokenizer(chunk).input_ids for chunk in text.split("<image>")]
            input_ids = torch.tensor(text_chunks[0] + [-200] + text_chunks[1], dtype=torch.long).unsqueeze(0)

            if self.device == "cuda":
                input_ids = input_ids.cuda()

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            image_tensor = self.model.process_images([image], self.model.config).to(dtype=self.model.dtype)
            if self.device == "cuda":
                image_tensor = image_tensor.cuda()

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            with torch.no_grad():
                output_ids = self.model.generate(
                    input_ids, images=image_tensor, max_new_tokens=200, use_cache=True, temperature=0.7, do_sample=True
                )[0]

            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            answer = self.tokenizer.decode(output_ids[input_ids.shape[1] :], skip_special_tokens=True).strip()

            logger.info(f"üìù nanoLLaVA: {answer[:100]}...")
            return answer

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
            return self._basic_description(image)

    def _basic_description(self, image):
        """–ë–∞–∑–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –±–µ–∑ ML –º–æ–¥–µ–ª–∏"""
        try:
            width, height = image.size
            mode = image.mode

            desc = f"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–º {width}x{height} –ø–∏–∫—Å–µ–ª–µ–π, {mode}"

            if mode == "L":
                desc += ", —á—ë—Ä–Ω–æ-–±–µ–ª–æ–µ"
            elif mode == "RGB":
                desc += ", —Ü–≤–µ—Ç–Ω–æ–µ"
            elif mode == "RGBA":
                desc += ", —Ü–≤–µ—Ç–Ω–æ–µ —Å –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç—å—é"

            return desc

        except Exception:
            return "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏"

    def unload_model(self):
        """–í—ã–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ –ø–∞–º—è—Ç–∏"""
        if self.model is not None:
            self.model = self.model.cpu()
            del self.model
            del self.tokenizer
            self.model = None
            self.tokenizer = None

            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()

            import gc

            gc.collect()
            logger.info("üßπ nanoLLaVA –≤—ã–≥—Ä—É–∂–µ–Ω –∏–∑ –ø–∞–º—è—Ç–∏")

    def is_model_loaded(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –ª–∏ –º–æ–¥–µ–ª—å"""
        return self.model is not None and self.tokenizer is not None
