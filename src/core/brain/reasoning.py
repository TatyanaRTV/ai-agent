"""
–ú–æ–¥—É–ª—å –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π
"""

import logging
from typing import Dict, Any, List, Optional
import json

logger = logging.getLogger(__name__)

class ReasoningEngine:
    """–î–≤–∏–∂–æ–∫ –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π"""
    
    def __init__(self, memory_manager):
        self.memory = memory_manager
        self.rules = self._load_rules()
        
    async def reason(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è"""
        logger.info("ü§î –ù–∞—á–∏–Ω–∞—é –ø—Ä–æ—Ü–µ—Å—Å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è...")
        
        reasoning_result = {
            "input_context": context,
            "conclusions": [],
            "confidence": 0.0,
            "reasoning_steps": [],
            "alternative_explanations": [],
            "timestamp": None
        }
        
        try:
            # 1. –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            analysis = await self._analyze_context(context)
            reasoning_result["reasoning_steps"].append(analysis)
            
            # 2. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª
            rule_conclusions = await self._apply_rules(context)
            reasoning_result["conclusions"].extend(rule_conclusions)
            
            # 3. –î–µ–¥—É–∫—Ç–∏–≤–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è
            deductive = await self._deductive_reasoning(context)
            reasoning_result["conclusions"].extend(deductive)
            
            # 4. –ò–Ω–¥—É–∫—Ç–∏–≤–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è
            inductive = await self._inductive_reasoning(context)
            reasoning_result["conclusions"].extend(inductive)
            
            # 5. –û—Ü–µ–Ω–∫–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
            confidence = await self._calculate_confidence(reasoning_result)
            reasoning_result["confidence"] = confidence
            
            # 6. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã—Ö –æ–±—ä—è—Å–Ω–µ–Ω–∏–π
            alternatives = await self._generate_alternatives(reasoning_result)
            reasoning_result["alternative_explanations"] = alternatives
            
            logger.info(f"‚úÖ –†–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2f}")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è: {e}")
            reasoning_result["error"] = str(e)
            
        return reasoning_result
        
    async def _analyze_context(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∑–∞–ø—Ä–æ—Å–∞"""
        analysis = {
            "step": "context_analysis",
            "user_intent": None,
            "emotional_tone": None,
            "complexity": None,
            "urgency": None
        }
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞–º–µ—Ä–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        if 'text' in context:
            text = context['text'].lower()
            
            # –ü—Ä–æ—Å—Ç—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –Ω–∞–º–µ—Ä–µ–Ω–∏–π
            intent_patterns = {
                'question': ['–∫–∞–∫', '—á—Ç–æ', '–≥–¥–µ', '–∫–æ–≥–¥–∞', '–ø–æ—á–µ–º—É', '–∑–∞—á–µ–º'],
                'command': ['—Å–¥–µ–ª–∞–π', '–Ω–∞–π–¥–∏', '–ø–æ–∫–∞–∂–∏', '—Ä–∞—Å—Å–∫–∞–∂–∏', '–≤–∫–ª—é—á–∏', '–≤—ã–∫–ª—é—á–∏'],
                'request': ['–ø–æ–º–æ–≥–∏', '–Ω—É–∂–Ω–æ', '–ø–æ–º–æ—â—å', '–º–æ–∂–µ—à—å'],
                'informational': ['–∑–Ω–∞–µ—à—å', '–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ', '—Ä–∞—Å—Å–∫–∞–∂–∏'],
                'social': ['–ø—Ä–∏–≤–µ—Ç', '–ø–æ–∫–∞', '—Å–ø–∞—Å–∏–±–æ', '–∏–∑–≤–∏–Ω–∏']
            }
            
            for intent, patterns in intent_patterns.items():
                if any(pattern in text for pattern in patterns):
                    analysis['user_intent'] = intent
                    break
                    
        # –û—Ü–µ–Ω–∫–∞ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ —Ç–æ–Ω–∞
        analysis['emotional_tone'] = await self._detect_emotion(context)
        
        # –û—Ü–µ–Ω–∫–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
        if 'text' in context:
            word_count = len(context['text'].split())
            if word_count < 5:
                analysis['complexity'] = 'low'
            elif word_count < 15:
                analysis['complexity'] = 'medium'
            else:
                analysis['complexity'] = 'high'
                
        # –û—Ü–µ–Ω–∫–∞ —Å—Ä–æ—á–Ω–æ—Å—Ç–∏
        urgency_words = ['—Å—Ä–æ—á–Ω–æ', '–±—ã—Å—Ç—Ä–æ', '–Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ', '—Å–∫–æ—Ä–µ–µ', '–ø–æ–±—ã—Å—Ç—Ä–µ–µ']
        if 'text' in context and any(word in context['text'].lower() for word in urgency_words):
            analysis['urgency'] = 'high'
        else:
            analysis['urgency'] = 'low'
            
        return analysis
        
    async def _apply_rules(self, context: Dict[str, Any]) -> List[Dict[str, Any]]:
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∞–≤–∏–ª"""
        conclusions = []
        
        for rule in self.rules:
            try:
                if await self._rule_matches(rule, context):
                    conclusion = {
                        "type": "rule_based",
                        "rule_id": rule.get("id"),
                        "description": rule.get("description"),
                        "conclusion": rule.get("conclusion"),
                        "confidence": rule.get("confidence", 0.7)
                    }
                    conclusions.append(conclusion)
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –ø—Ä–∞–≤–∏–ª–∞ {rule.get('id')}: {e}")
                
        return conclusions
        
    async def _deductive_reasoning(self, context: Dict[str, Any]) -> List[Dict[str, Any]]:
        """–î–µ–¥—É–∫—Ç–∏–≤–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è (–æ—Ç –æ–±—â–µ–≥–æ –∫ —á–∞—Å—Ç–Ω–æ–º—É)"""
        conclusions = []
        
        # –ü—Ä–∏–º–µ—Ä –¥–µ–¥—É–∫—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–∞–≤–∏–ª–∞: –µ—Å–ª–∏ A –∏ B, —Ç–æ C
        if 'text' in context:
            text = context['text'].lower()
            
            # –ü—Ä–∏–º–µ—Ä: –µ—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç –æ –ø–æ–≥–æ–¥–µ –∏ —Å–µ–π—á–∞—Å —É—Ç—Ä–æ, —Ç–æ –æ–Ω –ø–ª–∞–Ω–∏—Ä—É–µ—Ç –¥–µ–Ω—å
            if '–ø–æ–≥–æ–¥–∞' in text and self._is_morning():
                conclusions.append({
                    "type": "deductive",
                    "premise": "–ó–∞–ø—Ä–æ—Å –æ –ø–æ–≥–æ–¥–µ —É—Ç—Ä–æ–º",
                    "conclusion": "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø–ª–∞–Ω–∏—Ä—É–µ—Ç —Å–≤–æ–π –¥–µ–Ω—å",
                    "confidence": 0.8
                })
                
        return conclusions
        
    async def _inductive_reasoning(self, context: Dict[str, Any]) -> List[Dict[str, Any]]:
        """–ò–Ω–¥—É–∫—Ç–∏–≤–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è (–æ—Ç —á–∞—Å—Ç–Ω–æ–≥–æ –∫ –æ–±—â–µ–º—É)"""
        conclusions = []
        
        # –ü–æ–∏—Å–∫ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≤ –ø—Ä–æ—à–ª—ã—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è—Ö
        similar_interactions = await self.memory.find_similar_interactions(context, limit=5)
        
        if similar_interactions:
            # –ê–Ω–∞–ª–∏–∑ –æ–±—â–∏—Ö —á–µ—Ä—Ç
            common_patterns = self._find_common_patterns(similar_interactions)
            
            for pattern in common_patterns:
                conclusions.append({
                    "type": "inductive",
                    "pattern": pattern,
                    "conclusion": f"–ù–∞ –æ—Å–Ω–æ–≤–µ {len(similar_interactions)} –ø–æ—Ö–æ–∂–∏—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π",
                    "confidence": min(0.9, 0.5 + len(similar_interactions) * 0.1)
                })
                
        return conclusions
        
    async def _calculate_confidence(self, reasoning_result: Dict[str, Any]) -> float:
        """–†–∞—Å—á–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –≤ –≤—ã–≤–æ–¥–∞—Ö"""
        confidence = 0.5  # –ë–∞–∑–æ–≤–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        
        # –£—á–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –≤—ã–≤–æ–¥–æ–≤
        conclusions = reasoning_result.get("conclusions", [])
        if conclusions:
            avg_conclusion_confidence = sum(c.get("confidence", 0.5) for c in conclusions) / len(conclusions)
            confidence = (confidence + avg_conclusion_confidence) / 2
            
        # –£—á–µ—Ç –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã—Ö –æ–±—ä—è—Å–Ω–µ–Ω–∏–π
        alternatives = reasoning_result.get("alternative_explanations", [])
        if alternatives:
            confidence *= 0.9  # –ù–∞–ª–∏—á–∏–µ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤ —Å–Ω–∏–∂–∞–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
            
        return min(1.0, max(0.0, confidence))
        
    async def _generate_alternatives(self, reasoning_result: Dict[str, Any]) -> List[Dict[str, Any]]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã—Ö –æ–±—ä—è—Å–Ω–µ–Ω–∏–π"""
        alternatives = []
        context = reasoning_result.get("input_context", {})
        
        # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –Ω–∞–º–µ—Ä–µ–Ω–∏—è
        if 'text' in context:
            text = context['text'].lower()
            
            # –ü—Ä–∏–º–µ—Ä –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–π
            if '–ø–æ–≥–æ–¥–∞' in text:
                alternatives.append({
                    "interpretation": "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∏–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç—Å—è –ø–æ–≥–æ–¥–æ–π –¥–ª—è –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ–µ–∑–¥–∫–∏",
                    "confidence": 0.6
                })
                alternatives.append({
                    "interpretation": "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Ö–æ—á–µ—Ç –Ω–∞—á–∞—Ç—å —Ä–∞–∑–≥–æ–≤–æ—Ä —Å –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–π —Ç–µ–º—ã",
                    "confidence": 0.4
                })
                
        return alternatives
        
    def _load_rules(self) -> List[Dict[str, Any]]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∞–≤–∏–ª"""
        rules = [
            {
                "id": "rule_001",
                "description": "–ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∑–¥–æ—Ä–æ–≤–∞–µ—Ç—Å—è, —Ç–æ –Ω—É–∂–Ω–æ –æ—Ç–≤–µ—Ç–∏—Ç—å –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ–º",
                "condition": "any(word in text for word in ['–ø—Ä–∏–≤–µ—Ç', '–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π', '–¥–æ–±—Ä—ã–π –¥–µ–Ω—å'])",
                "conclusion": "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –æ–∂–∏–¥–∞–µ—Ç –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞",
                "confidence": 0.95
            },
            {
                "id": "rule_002",
                "description": "–ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç '–∫–∞–∫ –¥–µ–ª–∞', —ç—Ç–æ —Å–æ—Ü–∏–∞–ª—å–Ω—ã–π —Ä–∏—Ç—É–∞–ª",
                "condition": "'–∫–∞–∫ –¥–µ–ª–∞' in text or '–∫–∞–∫ —Ç—ã' in text",
                "conclusion": "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø—Ä–æ—è–≤–ª—è–µ—Ç –≤–µ–∂–ª–∏–≤–æ—Å—Ç—å, –∞ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏",
                "confidence": 0.85
            },
            {
                "id": "rule_003",
                "description": "–ï—Å–ª–∏ –∑–∞–ø—Ä–æ—Å —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–ª–æ–≤–æ '—Å—Ä–æ—á–Ω–æ', —Ç—Ä–µ–±—É–µ—Ç—Å—è –±—ã—Å—Ç—Ä—ã–π –æ—Ç–≤–µ—Ç",
                "condition": "any(word in text for word in ['—Å—Ä–æ—á–Ω–æ', '–±—ã—Å—Ç—Ä–æ', '–Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ'])",
                "conclusion": "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∏—Å–ø—ã—Ç—ã–≤–∞–µ—Ç —Å—Ä–æ—á–Ω–æ—Å—Ç—å, –Ω—É–∂–Ω–æ –æ—Ç–≤–µ—Ç–∏—Ç—å –±—ã—Å—Ç—Ä–æ",
                "confidence": 0.9
            }
        ]
        return rules
        
    async def _rule_matches(self, rule: Dict[str, Any], context: Dict[str, Any]) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –ø—Ä–∞–≤–∏–ª–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É"""
        try:
            if 'text' in context:
                text = context['text'].lower()
                condition = rule.get("condition", "")
                
                # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —É—Å–ª–æ–≤–∏–π (–º–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—É—é –ª–æ–≥–∏–∫—É)
                if "'–ø—Ä–∏–≤–µ—Ç'" in condition and any(word in text for word in ['–ø—Ä–∏–≤–µ—Ç', '–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π']):
                    return True
                if "'–∫–∞–∫ –¥–µ–ª–∞'" in condition and '–∫–∞–∫ –¥–µ–ª–∞' in text:
                    return True
                if "'—Å—Ä–æ—á–Ω–æ'" in condition and any(word in text for word in ['—Å—Ä–æ—á–Ω–æ', '–±—ã—Å—Ç—Ä–æ']):
                    return True
                    
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø—Ä–∞–≤–∏–ª–∞: {e}")
            
        return False
        
    async def _detect_emotion(self, context: Dict[str, Any]) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ —Ç–æ–Ω–∞"""
        if 'text' not in context:
            return 'neutral'
            
        text = context['text'].lower()
        
        positive_words = ['—Å–ø–∞—Å–∏–±–æ', '–æ—Ç–ª–∏—á–Ω–æ', '—Ö–æ—Ä–æ—à–æ', '–ø—Ä–µ–∫—Ä–∞—Å–Ω–æ', '—Ä–∞–¥', '–¥–æ–≤–æ–ª–µ–Ω']
        negative_words = ['–ø–ª–æ—Ö–æ', '—É–∂–∞—Å–Ω–æ', '–≥—Ä—É—Å—Ç–Ω–æ', '–∑–ª–æ–π', '—Ä–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω', '—Å–µ—Ä–¥–∏—Ç']
        
        positive_count = sum(1 for word in positive_words if word in text)
        negative_count = sum(1 for word in negative_words if word in text)
        
        if positive_count > negative_count:
            return 'positive'
        elif negative_count > positive_count:
            return 'negative'
        else:
            return 'neutral'
            
    def _is_morning(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, —Å–µ–π—á–∞—Å —É—Ç—Ä–æ –∏–ª–∏ –Ω–µ—Ç"""
        from datetime import datetime
        hour = datetime.now().hour
        return 6 <= hour < 12
        
    def _find_common_patterns(self, interactions: List[Dict[str, Any]]) -> List[str]:
        """–ü–æ–∏—Å–∫ –æ–±—â–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≤–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è—Ö"""
        patterns = []
        
        # –ü—Ä–æ—Å—Ç–∞—è –ª–æ–≥–∏–∫–∞ –ø–æ–∏—Å–∫–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        texts = [interaction.get('text', '').lower() for interaction in interactions if 'text' in interaction]
        
        # –ü–æ–∏—Å–∫ –æ–±—â–∏—Ö —Å–ª–æ–≤
        from collections import Counter
        all_words = []
        for text in texts:
            all_words.extend(text.split())
            
        word_counts = Counter(all_words)
        common_words = [word for word, count in word_counts.items() if count > 1 and len(word) > 3]
        
        if common_words:
            patterns.append(f"–ß–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Å–ª–æ–≤–∞: {', '.join(common_words[:3])}")
            
        return patterns