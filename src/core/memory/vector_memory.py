"""
–í–µ–∫—Ç–æ—Ä–Ω–∞—è –ø–∞–º—è—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ ChromaDB
"""

import chromadb
from chromadb.config import Settings
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
import json
import hashlib
import os
import shutil

logger = logging.getLogger(__name__)

class VectorMemory:
    """–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ø–∞–º—è—Ç—å—é"""
    
    def __init__(self, persist_directory: str = "./data/vectors"):
        self.persist_directory = persist_directory
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ChromaDB - –ù–û–í–ê–Ø –í–ï–†–°–ò–Ø
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º PersistentClient –≤–º–µ—Å—Ç–æ —É—Å—Ç–∞—Ä–µ–≤—à–µ–≥–æ Client
            self.client = chromadb.PersistentClient(
                path=persist_directory
            )
            
            # –ö–æ–ª–ª–µ–∫—Ü–∏—è –¥–ª—è –æ–±—â–∏—Ö –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏–π
            self.memories_collection = self.client.get_or_create_collection(
                name="memories",
                metadata={"description": "–î–æ–ª–≥–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è –ø–∞–º—è—Ç—å –∞–≥–µ–Ω—Ç–∞", "hnsw:space": "cosine"}
            )
            
            # –ö–æ–ª–ª–µ–∫—Ü–∏—è –¥–ª—è –∑–Ω–∞–Ω–∏–π
            self.knowledge_collection = self.client.get_or_create_collection(
                name="knowledge",
                metadata={"description": "–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –∞–≥–µ–Ω—Ç–∞", "hnsw:space": "cosine"}
            )
            
            # –ö–æ–ª–ª–µ–∫—Ü–∏—è –¥–ª—è –æ–ø—ã—Ç–∞
            self.experience_collection = self.client.get_or_create_collection(
                name="experience",
                metadata={"description": "–û–ø—ã—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π", "hnsw:space": "cosine"}
            )
            
            logger.info("üß† –í–µ–∫—Ç–æ—Ä–Ω–∞—è –ø–∞–º—è—Ç—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ (PersistentClient)")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ ChromaDB: {e}")
            raise
        
    def store_memory(self, content: str, metadata: Dict[str, Any] = None, 
                    embedding: Optional[List[float]] = None) -> str:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏—è"""
        memory_id = hashlib.md5(f"{content}{datetime.now().isoformat()}".encode()).hexdigest()
        
        if metadata is None:
            metadata = {}
            
        metadata.update({
            "timestamp": datetime.now().isoformat(),
            "type": "memory",
            "source": "agent"
        })
        
        try:
            self.memories_collection.add(
                documents=[content],
                metadatas=[metadata],
                ids=[memory_id],
                embeddings=[embedding] if embedding else None
            )
            
            logger.debug(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏–µ: {memory_id}")
            return memory_id
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏—è: {e}")
            raise
            
    def store_knowledge(self, content: str, category: str, 
                       metadata: Dict[str, Any] = None) -> str:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏—è"""
        knowledge_id = hashlib.md5(f"{content}{category}".encode()).hexdigest()
        
        if metadata is None:
            metadata = {}
            
        metadata.update({
            "category": category,
            "timestamp": datetime.now().isoformat(),
            "verified": True,
            "confidence": 0.9
        })
        
        try:
            self.knowledge_collection.add(
                documents=[content],
                metadatas=[metadata],
                ids=[knowledge_id]
            )
            
            logger.debug(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –∑–Ω–∞–Ω–∏–µ: {knowledge_id}")
            return knowledge_id
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∑–Ω–∞–Ω–∏—è: {e}")
            raise
            
    def store_experience(self, interaction_data: Dict[str, Any]) -> str:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–ø—ã—Ç–∞ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è"""
        experience_id = hashlib.md5(
            f"{json.dumps(interaction_data)}{datetime.now().isoformat()}".encode()
        ).hexdigest()
        
        metadata = {
            "timestamp": datetime.now().isoformat(),
            "type": "experience",
            "success_rate": interaction_data.get("success_rate", 0.5),
            "learning_outcome": interaction_data.get("learning_outcome", "")
        }
        
        # –°–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è
        content = json.dumps(interaction_data, ensure_ascii=False)
        
        try:
            self.experience_collection.add(
                documents=[content],
                metadatas=[metadata],
                ids=[experience_id]
            )
            
            logger.debug(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω –æ–ø—ã—Ç: {experience_id}")
            return experience_id
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ–ø—ã—Ç–∞: {e}")
            raise
            
    def search_memories(self, query: str, limit: int = 10, 
                       threshold: float = 0.3) -> List[Dict[str, Any]]:
        """–ü–æ–∏—Å–∫ –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏–π"""
        try:
            results = self.memories_collection.query(
                query_texts=[query],
                n_results=limit
            )
            
            memories = []
            if results['documents'] and results['documents'][0]:
                for i, doc in enumerate(results['documents'][0]):
                    memories.append({
                        "content": doc,
                        "metadata": results['metadatas'][0][i] if results['metadatas'] else {},
                        "distance": results['distances'][0][i] if results['distances'] else 0,
                        "id": results['ids'][0][i]
                    })
                    
            return memories
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏–π: {e}")
            return []
            
    def search_knowledge(self, query: str, category: Optional[str] = None, 
                        limit: int = 5) -> List[Dict[str, Any]]:
        """–ü–æ–∏—Å–∫ –∑–Ω–∞–Ω–∏–π"""
        try:
            where_filter = None
            if category:
                where_filter = {"category": category}
                
            results = self.knowledge_collection.query(
                query_texts=[query],
                n_results=limit,
                where=where_filter
            )
            
            knowledge_items = []
            if results['documents'] and results['documents'][0]:
                for i, doc in enumerate(results['documents'][0]):
                    knowledge_items.append({
                        "content": doc,
                        "metadata": results['metadatas'][0][i] if results['metadatas'] else {},
                        "distance": results['distances'][0][i] if results['distances'] else 0,
                        "id": results['ids'][0][i]
                    })
                
            return knowledge_items
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –∑–Ω–∞–Ω–∏–π: {e}")
            return []
            
    def get_similar_experiences(self, current_experience: Dict[str, Any], 
                               limit: int = 3) -> List[Dict[str, Any]]:
        """–ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–µ–≥–æ –æ–ø—ã—Ç–∞"""
        try:
            # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –∏–∑ –æ–ø—ã—Ç–∞
            query_text = json.dumps(current_experience, ensure_ascii=False)
            
            results = self.experience_collection.query(
                query_texts=[query_text],
                n_results=limit
            )
            
            experiences = []
            if results['documents'] and results['documents'][0]:
                for i, doc in enumerate(results['documents'][0]):
                    try:
                        experience_data = json.loads(doc)
                        experiences.append({
                            "data": experience_data,
                            "metadata": results['metadatas'][0][i] if results['metadatas'] else {},
                            "similarity": 1 - (results['distances'][0][i] if results['distances'] else 0),
                            "id": results['ids'][0][i]
                        })
                    except json.JSONDecodeError:
                        continue
                    
            return experiences
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –ø–æ—Ö–æ–∂–µ–≥–æ –æ–ø—ã—Ç–∞: {e}")
            return []
            
    def update_memory(self, memory_id: str, new_content: str, 
                     new_metadata: Dict[str, Any] = None):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏—è"""
        try:
            current = self.memories_collection.get(ids=[memory_id])
            if not current['documents']:
                raise ValueError(f"–í–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏–µ {memory_id} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
                
            metadata = current['metadatas'][0] if current['metadatas'] else {}
            if new_metadata:
                metadata.update(new_metadata)
                
            metadata['updated_at'] = datetime.now().isoformat()
            metadata['update_count'] = metadata.get('update_count', 0) + 1
            
            # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä–æ–π –∑–∞–ø–∏—Å–∏
            self.memories_collection.delete(ids=[memory_id])
            
            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –æ–±–Ω–æ–≤–ª–µ–Ω–Ω–æ–π –∑–∞–ø–∏—Å–∏
            self.memories_collection.add(
                documents=[new_content],
                metadatas=[metadata],
                ids=[memory_id]
            )
            
            logger.debug(f"–û–±–Ω–æ–≤–ª–µ–Ω–æ –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏–µ: {memory_id}")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏—è: {e}")
            raise
            
    def delete_memory(self, memory_id: str):
        """–£–¥–∞–ª–µ–Ω–∏–µ –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏—è"""
        try:
            self.memories_collection.delete(ids=[memory_id])
            logger.debug(f"–£–¥–∞–ª–µ–Ω–æ –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏–µ: {memory_id}")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏—è: {e}")
            raise
            
    def get_memory_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–∞–º—è—Ç–∏"""
        try:
            memory_count = self.memories_collection.count()
            knowledge_count = self.knowledge_collection.count()
            experience_count = self.experience_collection.count()
            
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–æ–≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∑–Ω–∞–Ω–∏–π
            categories = set()
            try:
                knowledge_samples = self.knowledge_collection.get(limit=5)
                for meta in knowledge_samples['metadatas']:
                    if meta and 'category' in meta:
                        categories.add(meta['category'])
            except:
                pass
                    
            return {
                "total_memories": memory_count,
                "total_knowledge": knowledge_count,
                "total_experience": experience_count,
                "knowledge_categories": list(categories),
                "memory_size": self._estimate_memory_size(),
                "last_backup": self._get_last_backup_time()
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–∞–º—è—Ç–∏: {e}")
            return {}
            
    def _estimate_memory_size(self) -> str:
        """–û—Ü–µ–Ω–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ –ø–∞–º—è—Ç–∏"""
        total_size = 0
        if os.path.exists(self.persist_directory):
            for dirpath, dirnames, filenames in os.walk(self.persist_directory):
                for f in filenames:
                    fp = os.path.join(dirpath, f)
                    total_size += os.path.getsize(fp)
                    
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —á–∏—Ç–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç
        for unit in ['B', 'KB', 'MB', 'GB']:
            if total_size < 1024:
                return f"{total_size:.2f} {unit}"
            total_size /= 1024
            
        return f"{total_size:.2f} TB"
        
    def _get_last_backup_time(self) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –±—ç–∫–∞–ø–∞"""
        backup_file = f"{self.persist_directory}/backup_timestamp.txt"
        
        try:
            if os.path.exists(backup_file):
                with open(backup_file, 'r') as f:
                    return f.read().strip()
        except:
            pass
            
        return "–Ω–∏–∫–æ–≥–¥–∞"
        
    def create_backup(self, backup_path: str = None):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏ –ø–∞–º—è—Ç–∏"""
        if backup_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_path = f"./data/backups/memory_backup_{timestamp}"
            
        try:
            # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –±—ç–∫–∞–ø–∞
            os.makedirs(os.path.dirname(backup_path), exist_ok=True)
            
            # –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            shutil.copytree(self.persist_directory, backup_path)
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –±—ç–∫–∞–ø–∞
            backup_meta = {
                "timestamp": datetime.now().isoformat(),
                "source": self.persist_directory,
                "destination": backup_path,
                "stats": self.get_memory_stats()
            }
            
            meta_file = f"{backup_path}/backup_metadata.json"
            with open(meta_file, 'w', encoding='utf-8') as f:
                json.dump(backup_meta, f, ensure_ascii=False, indent=2)
                
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –±—ç–∫–∞–ø–∞
            os.makedirs(self.persist_directory, exist_ok=True)
            timestamp_file = f"{self.persist_directory}/backup_timestamp.txt"
            with open(timestamp_file, 'w') as f:
                f.write(datetime.now().isoformat())
                
            logger.info(f"‚úÖ –†–µ–∑–µ—Ä–≤–Ω–∞—è –∫–æ–ø–∏—è —Å–æ–∑–¥–∞–Ω–∞: {backup_path}")
            return backup_path
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏: {e}")
            raise
            
    def restore_from_backup(self, backup_path: str):
        """–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏ –∏–∑ —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏"""
        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è –±—ç–∫–∞–ø–∞
            if not os.path.exists(backup_path):
                raise ValueError(f"–ü—É—Ç—å –±—ç–∫–∞–ø–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {backup_path}")
                
            # –°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏ —Ç–µ–∫—É—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
            temp_backup = f"{self.persist_directory}_temp_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            if os.path.exists(self.persist_directory):
                shutil.move(self.persist_directory, temp_backup)
                
            # –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –±—ç–∫–∞–ø–∞
            shutil.copytree(backup_path, self.persist_directory)
            
            logger.info(f"‚úÖ –ü–∞–º—è—Ç—å –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –∏–∑ –±—ç–∫–∞–ø–∞: {backup_path}")
            
            # –£–¥–∞–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∫–æ–ø–∏–∏ –ø–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–≥–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è
            import time
            time.sleep(1)
            if os.path.exists(temp_backup):
                shutil.rmtree(temp_backup)
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –∏–∑ –±—ç–∫–∞–ø–∞: {e}")
            
            # –ü–æ–ø—ã—Ç–∫–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            if 'temp_backup' in locals() and os.path.exists(temp_backup):
                if os.path.exists(self.persist_directory):
                    shutil.rmtree(self.persist_directory)
                shutil.move(temp_backup, self.persist_directory)
                
            raise
            
    def cleanup_old_memories(self, days_old: int = 90):
        """–û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏–π"""
        try:
            cutoff_date = (datetime.now() - timedelta(days=days_old)).isoformat()
            
            # –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏–π
            old_memories = self.memories_collection.get(
                where={"timestamp": {"$lt": cutoff_date}}
            )
            
            if old_memories['ids']:
                # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏–π
                self.memories_collection.delete(ids=old_memories['ids'])
                logger.info(f"–£–¥–∞–ª–µ–Ω–æ {len(old_memories['ids'])} —Å—Ç–∞—Ä—ã—Ö –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏–π")
                
            return len(old_memories['ids'])
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ —Å—Ç–∞—Ä—ã—Ö –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏–π: {e}")
            return 0
            
    def export_memories(self, export_path: str, format: str = "json"):
        """–≠–∫—Å–ø–æ—Ä—Ç –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏–π"""
        try:
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏–π
            all_memories = self.memories_collection.get(
                include=["documents", "metadatas", "ids"]
            )
            
            export_data = {
                "export_timestamp": datetime.now().isoformat(),
                "total_memories": len(all_memories['ids']),
                "memories": []
            }
            
            for i, memory_id in enumerate(all_memories['ids']):
                export_data["memories"].append({
                    "id": memory_id,
                    "content": all_memories['documents'][i],
                    "metadata": all_memories['metadatas'][i] if all_memories['metadatas'] else {}
                })
                
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
            os.makedirs(os.path.dirname(export_path), exist_ok=True)
            
            if format.lower() == "json":
                with open(f"{export_path}.json", 'w', encoding='utf-8') as f:
                    json.dump(export_data, f, ensure_ascii=False, indent=2)
            elif format.lower() == "csv":
                import csv
                with open(f"{export_path}.csv", 'w', newline='', encoding='utf-8') as f:
                    writer = csv.writer(f)
                    writer.writerow(['ID', 'Content', 'Timestamp', 'Type'])
                    for memory in export_data["memories"]:
                        writer.writerow([
                            memory['id'],
                            memory['content'][:100] + "..." if len(memory['content']) > 100 else memory['content'],
                            memory['metadata'].get('timestamp', ''),
                            memory['metadata'].get('type', '')
                        ])
            else:
                raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç: {format}")
                
            logger.info(f"–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ {len(export_data['memories'])} –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏–π –≤ {export_path}.{format}")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏–π: {e}")
            raise